{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "Pbcc9QS1tnBN",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# ASSIGNMENT 4 - Classification Empirical Study: Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2Eeke4Z_EkW"
   },
   "source": [
    "## Group Description\n",
    "\n",
    "Group Number: 25\n",
    "\n",
    "Member Name 1: Jake Wang\n",
    "\n",
    "Member Student Number 1: ***REMOVED***\n",
    "\n",
    "Member Name 2: Victor Li\n",
    "\n",
    "Member Student Number 2: ***REMOVED***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMUnCICdyBbs"
   },
   "source": [
    "## Derived Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-so3pwbPKTDX"
   },
   "source": [
    "This notebook is a starting point for Assignment 4. In this assignment, you will perform a classification empirical study. This notebook will help you to create derived datasets in Section 2 of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhFvS3q7Lu0R"
   },
   "outputs": [],
   "source": [
    "# Let's start by installing spaCy\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCWgl6PLKTDY"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EX9WQWGSwU2D"
   },
   "source": [
    "You have been given a list of datasets in the assignment description. Choose one of the datasets and provide the link below and read the dataset using pandas. You should provide a link to your own Github repository even if you are using a reduced version of a dataset from your TA's repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSyg0jjC1jJa"
   },
   "source": [
    "<span style=\"color: #f00; font-size: xx-large\">TODO: add description of the dataset and your justification of the choices made to obtain the derived datasets</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Xx4qMCLKTDb"
   },
   "outputs": [],
   "source": [
    "# url = \"https://raw.githubusercontent.com/uOttawa-Collabs/CSI4106-Fall-2023/master/Assignment%204/reduced_drugsComTest_raw_fiveclasses.csv\"\n",
    "url = \"https://raw.githubusercontent.com/uOttawa-Collabs/CSI4106-Fall-2023/master/Assignment%204/reduced_file_AirPassengerReviews.csv\"\n",
    "# url = \"https://raw.githubusercontent.com/uOttawa-Collabs/CSI4106-Fall-2023/master/Assignment%204/reduced_file_cnnnews.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wg24OUV81Xgm"
   },
   "outputs": [],
   "source": [
    "print(url)\n",
    "data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQ5nSY1HKTDd"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3pycllPKTDd"
   },
   "source": [
    "This is where you create the NLP pipeline. load() will download the correct model (English)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQtSi8XuKTDe"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccqFArDyKTDf"
   },
   "source": [
    "Applying the pipeline to every sentences creates a Document where every word is a Token object.\n",
    "\n",
    "Doc: https://spacy.io/api/doc\n",
    "\n",
    "Token: https://spacy.io/api/token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WcaeMUL2KTDg"
   },
   "outputs": [],
   "source": [
    "# Apply nlp pipeline to the column that has your sentences.\n",
    "data[\"tokenized\"] = data[\"customer_review\"].apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7i6ai1I8KTDg"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHRfZ2uEKTDh"
   },
   "source": [
    "A Token object has many attributes such as part-of-speech (pos_), lemma (lemma_), etc. Take a look at the documentation to see all attributes.\n",
    "\n",
    "The following function is an example on how you can fetch a specific pos tagging from a sentence. We return the lemmatization because we only want the infinitive word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qw0a_2ySyUo2"
   },
   "outputs": [],
   "source": [
    "# Create empty dataframes that will store your derived datasets\n",
    "derived_dataset1 = pd.DataFrame(columns=[\"Class\", \"pos\"])\n",
    "derived_dataset2 = pd.DataFrame(columns=[\"Class\", \"pos-np\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yeak1tAOKTDi"
   },
   "outputs": [],
   "source": [
    "def get_pos(sentence, wanted_pos):  # wanted_pos refers to the desired pos tagging\n",
    "    words = []\n",
    "    for token in sentence:\n",
    "        if token.pos_ in wanted_pos:\n",
    "            words.append(token.lemma_)  # lemma returns a number. lemma_ return a string\n",
    "    return \" \".join(words)  # return value is as a string and not a list for countVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #f00; font-size: xx-large\">TODO: Explain the choice of wanted POS.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "147NRzwKKTDj"
   },
   "outputs": [],
   "source": [
    "derived_dataset1[\"Class\"] = data[\"NPS Score\"]\n",
    "derived_dataset1[\"pos\"] = data[\"tokenized\"].apply(lambda s: get_pos(s, [\"NOUN\", \"VERB\", \"ADJ\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_bUg_fVKTDk"
   },
   "outputs": [],
   "source": [
    "derived_dataset1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ne(sentence):\n",
    "    words = []\n",
    "    for entity in sentence.ents:\n",
    "        words.append(entity.text)\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuGv-NnfKTDj"
   },
   "outputs": [],
   "source": [
    "# Change this line to fetch your desired pos taggings for the second derived dataset\n",
    "derived_dataset2[\"Class\"] = data[\"NPS Score\"]\n",
    "derived_dataset2[\"pos-np\"] = data[\"tokenized\"].apply(lambda s: get_ne(s) + \" \" + get_pos(s, [\"VERB\", \"ADJ\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derived_dataset2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NR7AdW0MfXO6"
   },
   "outputs": [],
   "source": [
    "# For Derived Dataset 2, you also need to include Named Entities\n",
    "# Below is just an example of obtaining such entities on a specific sentence, but you would do NER\n",
    "#   on the dataset of your choice.\n",
    "# You can choose the types of entities (dates, organization, people) that you want,\n",
    "#   and then in your derived dataset, just make sure you include these entities separated by spaces (as shown for verbs)\n",
    "#   in a previous cell.\n",
    "\n",
    "sentence = \"apple is looking at buying U.K. startup for $1 billion\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pX4RgKhKTDk"
   },
   "source": [
    "Now that you have your derived datasets, you can move to perform your classificaton task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GhniwHtzfQt"
   },
   "source": [
    "## Perform Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the text as input features with associated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "data_vectorized = tfidf_vectorizer.fit_transform(data[\"tokenized\"].apply(str))\n",
    "derived_dataset1_vectorized = tfidf_vectorizer.fit_transform(derived_dataset1[\"pos\"])\n",
    "derived_dataset2_vectorized = tfidf_vectorizer.fit_transform(derived_dataset2[\"pos-np\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define 2 models using some default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "default_logistic_regression = LogisticRegression()\n",
    "default_multilayer_perceptron = MLPClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test/evaluate the 2 models with default parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Initialize KFold Cross Validator***\n",
    "\n",
    "Here we are initializing the `KFold` validator. Explanation of parameters:\n",
    "* `n_splits=4` means we are using 4-fold validation.\n",
    "* `shuffle=True` means data will be shuffled before spliting to batches.\n",
    "* `random_state` is the random seed used for shuffling. Here we are using a fixed number to keep reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "four_fold = KFold(n_splits=4, shuffle=True, random_state=4106)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Training and Evaluating the Model***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Utility Functions****\n",
    "\n",
    "Description for core function `train_and_evaluate`:\n",
    "\n",
    "Inputs:\n",
    "* `model`: The machine learning model to be trained and evaluated.\n",
    "* `validator`: A cross-validator object used for splitting the data into training and testing sets.\n",
    "* `csr_matrix_training`: A scipy `csr_matrix` containing input features for training the model.\n",
    "* `series_validating`: A pandas `Series` of target labels for validating the model's predictions.\n",
    "\n",
    "The function iterates through the training and testing sets created by the `validator`.\n",
    "1. For each iteration, it trains the model using the training sets.\n",
    "2. Then, it makes predictions on the test data.\n",
    "3. Precision and recall scores are calculated for both micro and macro averages using the `precision_score` and `recall_score` functions from scikit-learn.\n",
    "4. Zero division is handled by setting `zero_division=0`.\n",
    "\n",
    "The calculated precision and recall scores are appended to respective lists.\n",
    "\n",
    "After iterating through all folds, the function calculates the average precision and recall scores for both micro and macro averages using the helper function average.\n",
    "The function returns a tuple containing the average micro precision, average macro precision, average micro recall, and average macro recall scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def train_and_evaluate(model, validator, csr_matrix_training, series_validating):\n",
    "    micro_precision_scores = []\n",
    "    macro_precision_scores = []\n",
    "    micro_recall_scores = []\n",
    "    macro_recall_scores = []\n",
    "\n",
    "    X = csr_matrix_training\n",
    "    y = series_validating\n",
    "\n",
    "    # Split the dataset into training set and testing set with validator\n",
    "    for train_index, test_index in validator.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions Å·\n",
    "        y_hat = model.predict(X_test)\n",
    "\n",
    "        # Calculate precision and recall for micro and macro averages\n",
    "        micro_precision = precision_score(y_test, y_hat, average=\"micro\", zero_division=0)\n",
    "        macro_precision = precision_score(y_test, y_hat, average=\"macro\", zero_division=0)\n",
    "        micro_recall = recall_score(y_test, y_hat, average=\"micro\", zero_division=0)\n",
    "        macro_recall = recall_score(y_test, y_hat, average=\"macro\", zero_division=0)\n",
    "\n",
    "        # Append scores to lists\n",
    "        if micro_precision != 0:\n",
    "            micro_precision_scores.append(micro_precision)\n",
    "        if macro_precision != 0:\n",
    "            macro_precision_scores.append(macro_precision)\n",
    "        if micro_recall != 0:\n",
    "            micro_recall_scores.append(micro_recall)\n",
    "        if macro_recall != 0:\n",
    "            macro_recall_scores.append(macro_recall)\n",
    "\n",
    "    # Calculate average precision and recall scores for micro and macro averages\n",
    "    return (\n",
    "        average(micro_precision_scores),\n",
    "        average(macro_precision_scores),\n",
    "        average(micro_recall_scores),\n",
    "        average(macro_recall_scores)\n",
    "    )\n",
    "\n",
    "\n",
    "def average(numeric_list):\n",
    "    return sum(numeric_list) / len(numeric_list)\n",
    "\n",
    "\n",
    "def print_result(\n",
    "    average_micro_precision,\n",
    "    average_macro_precision,\n",
    "    average_micro_recall,\n",
    "    average_macro_recall\n",
    "):\n",
    "    print(\"Average Micro Precision: {:.2f}\".format(average_micro_precision))\n",
    "    print(\"Average Macro Precision: {:.2f}\".format(average_macro_precision))\n",
    "    print(\"Average Micro Recall: {:.2f}\".format(average_micro_recall))\n",
    "    print(\"Average Macro Recall: {:.2f}\".format(average_macro_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Logistic Regression on Original Dataset****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_lr_original_evaluation = train_and_evaluate(\n",
    "    default_logistic_regression,\n",
    "    four_fold, data_vectorized,\n",
    "    data[\"NPS Score\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *default_lr_original_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Multilayer Perceptron on Original Dataset****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_mlp_original_evaluation = train_and_evaluate(\n",
    "    default_multilayer_perceptron,\n",
    "    four_fold, data_vectorized,\n",
    "    data[\"NPS Score\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *default_mlp_original_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Logistic Regression on Derived Dataset #1****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_lr_derived_1_evaluation = train_and_evaluate(\n",
    "    default_logistic_regression,\n",
    "    four_fold, derived_dataset1_vectorized,\n",
    "    derived_dataset1[\"Class\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *default_lr_derived_1_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Multilayer Perceptron on Derived Dataset #1****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_mlp_derived_1_evaluation = train_and_evaluate(\n",
    "    default_multilayer_perceptron,\n",
    "    four_fold, derived_dataset1_vectorized,\n",
    "    derived_dataset1[\"Class\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *default_mlp_derived_1_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Logistic Regression on Derived Dataset #2****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_lr_derived_2_evaluation = train_and_evaluate(\n",
    "    default_logistic_regression,\n",
    "    four_fold, derived_dataset2_vectorized,\n",
    "    derived_dataset2[\"Class\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *default_lr_derived_2_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Multilayer Perceptron on Derived Dataset #2****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_mlp_derived_2_evaluation = train_and_evaluate(\n",
    "    default_multilayer_perceptron,\n",
    "    four_fold, derived_dataset2_vectorized,\n",
    "    derived_dataset2[\"Class\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *default_mlp_derived_2_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test/evaluate the 2 models with modified parameters (#1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test/evaluate the 2 models with modified parameters (#2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the obtained results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "CSI4106-Assignment-4-venv",
   "language": "python",
   "name": "csi4106-assignment-4-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
