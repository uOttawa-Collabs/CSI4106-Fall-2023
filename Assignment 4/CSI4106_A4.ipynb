{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "Pbcc9QS1tnBN",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# ASSIGNMENT 4 - Classification Empirical Study: Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2Eeke4Z_EkW"
   },
   "source": [
    "## Group Description\n",
    "\n",
    "Group Number: 25\n",
    "\n",
    "Member Name 1: Xiaoxuan Wang\n",
    "\n",
    "Member Student Number 1: 300133594\n",
    "\n",
    "Member Name 2: Victor Li\n",
    "\n",
    "Member Student Number 2: 300146133"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMUnCICdyBbs"
   },
   "source": [
    "## Derived Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-so3pwbPKTDX"
   },
   "source": [
    "This notebook is a starting point for Assignment 4. In this assignment, you will perform a classification empirical study. This notebook will help you to create derived datasets in Section 2 of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhFvS3q7Lu0R"
   },
   "outputs": [],
   "source": [
    "# Let's start by installing spaCy\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aCWgl6PLKTDY"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EX9WQWGSwU2D"
   },
   "source": [
    "You have been given a list of datasets in the assignment description. Choose one of the datasets and provide the link below and read the dataset using pandas. You should provide a link to your own Github repository even if you are using a reduced version of a dataset from your TA's repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSyg0jjC1jJa"
   },
   "source": [
    "<span style=\"color: #f00; font-size: xx-large\">TODO: add description of the dataset and your justification of the choices made to obtain the derived datasets</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1Xx4qMCLKTDb"
   },
   "outputs": [],
   "source": [
    "# url = \"https://raw.githubusercontent.com/uOttawa-Collabs/CSI4106-Fall-2023/master/Assignment%204/reduced_drugsComTest_raw_fiveclasses.csv\"\n",
    "url = \"https://raw.githubusercontent.com/uOttawa-Collabs/CSI4106-Fall-2023/master/Assignment%204/reduced_file_AirPassengerReviews.csv\"\n",
    "# url = \"https://raw.githubusercontent.com/uOttawa-Collabs/CSI4106-Fall-2023/master/Assignment%204/reduced_file_cnnnews.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wg24OUV81Xgm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://raw.githubusercontent.com/uOttawa-Collabs/CSI4106-Fall-2023/master/Assignment%204/reduced_file_AirPassengerReviews.csv\n"
     ]
    }
   ],
   "source": [
    "print(url)\n",
    "data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AQ5nSY1HKTDd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_review</th>\n",
       "      <th>NPS Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>London to Izmir via Istanbul. First time I'd ...</td>\n",
       "      <td>Passive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Istanbul to Bucharest. We make our check in i...</td>\n",
       "      <td>Detractor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rome to Prishtina via Istanbul. I flew with t...</td>\n",
       "      <td>Detractor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Flew on Turkish Airlines IAD-IST-KHI and retu...</td>\n",
       "      <td>Promoter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mumbai to Dublin via Istanbul. Never book Tur...</td>\n",
       "      <td>Detractor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     customer_review  NPS Score\n",
       "0   London to Izmir via Istanbul. First time I'd ...    Passive\n",
       "1   Istanbul to Bucharest. We make our check in i...  Detractor\n",
       "2   Rome to Prishtina via Istanbul. I flew with t...  Detractor\n",
       "3   Flew on Turkish Airlines IAD-IST-KHI and retu...   Promoter\n",
       "4   Mumbai to Dublin via Istanbul. Never book Tur...  Detractor"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3pycllPKTDd"
   },
   "source": [
    "This is where you create the NLP pipeline. load() will download the correct model (English)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wQtSi8XuKTDe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.1/12.8 MB 2.2 MB/s eta 0:00:06\n",
      "      --------------------------------------- 0.2/12.8 MB 1.8 MB/s eta 0:00:08\n",
      "      --------------------------------------- 0.3/12.8 MB 1.8 MB/s eta 0:00:08\n",
      "     - -------------------------------------- 0.3/12.8 MB 1.8 MB/s eta 0:00:07\n",
      "     - -------------------------------------- 0.4/12.8 MB 1.8 MB/s eta 0:00:08\n",
      "     - -------------------------------------- 0.5/12.8 MB 1.9 MB/s eta 0:00:07\n",
      "     - -------------------------------------- 0.6/12.8 MB 1.8 MB/s eta 0:00:07\n",
      "     -- ------------------------------------- 0.7/12.8 MB 1.9 MB/s eta 0:00:07\n",
      "     -- ------------------------------------- 0.8/12.8 MB 1.9 MB/s eta 0:00:07\n",
      "     -- ------------------------------------- 0.9/12.8 MB 2.0 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.0/12.8 MB 2.0 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.2/12.8 MB 2.0 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.3/12.8 MB 2.1 MB/s eta 0:00:06\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 2.1 MB/s eta 0:00:06\n",
      "     ---- ----------------------------------- 1.5/12.8 MB 2.1 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 1.6/12.8 MB 2.1 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 1.7/12.8 MB 2.2 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 1.9/12.8 MB 2.2 MB/s eta 0:00:05\n",
      "     ------ --------------------------------- 2.0/12.8 MB 2.2 MB/s eta 0:00:05\n",
      "     ------ --------------------------------- 2.1/12.8 MB 2.3 MB/s eta 0:00:05\n",
      "     ------- -------------------------------- 2.3/12.8 MB 2.3 MB/s eta 0:00:05\n",
      "     ------- -------------------------------- 2.4/12.8 MB 2.3 MB/s eta 0:00:05\n",
      "     ------- -------------------------------- 2.5/12.8 MB 2.4 MB/s eta 0:00:05\n",
      "     -------- ------------------------------- 2.7/12.8 MB 2.4 MB/s eta 0:00:05\n",
      "     -------- ------------------------------- 2.8/12.8 MB 2.4 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 3.0/12.8 MB 2.4 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 3.1/12.8 MB 2.4 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 3.3/12.8 MB 2.5 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 2.5 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 3.6/12.8 MB 2.5 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 2.6 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 3.9/12.8 MB 2.6 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 4.0/12.8 MB 2.6 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 4.2/12.8 MB 2.6 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 4.3/12.8 MB 2.6 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 4.5/12.8 MB 2.7 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 4.7/12.8 MB 2.7 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 4.8/12.8 MB 2.7 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 5.0/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 5.4/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 5.6/12.8 MB 2.9 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 5.8/12.8 MB 2.9 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 6.0/12.8 MB 2.9 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 6.1/12.8 MB 2.9 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 6.3/12.8 MB 3.0 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 6.5/12.8 MB 3.0 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 6.7/12.8 MB 3.0 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 6.9/12.8 MB 3.0 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.2/12.8 MB 3.1 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.4/12.8 MB 3.1 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.7/12.8 MB 3.1 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 7.9/12.8 MB 3.2 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.1/12.8 MB 3.2 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.4/12.8 MB 3.3 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.6/12.8 MB 3.3 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.8/12.8 MB 3.3 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.1/12.8 MB 3.4 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 3.4 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 3.4 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.6/12.8 MB 3.4 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.8/12.8 MB 3.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.0/12.8 MB 3.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.1/12.8 MB 3.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.3/12.8 MB 3.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.5/12.8 MB 3.5 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 3.6 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 10.9/12.8 MB 3.6 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.1/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.5/12.8 MB 3.8 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.6/12.8 MB 3.8 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.4/12.8 MB 4.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 4.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 4.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 4.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\python311\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.2)\n",
      "Requirement already satisfied: jinja2 in c:\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\r1060\\appdata\\roaming\\python\\python311\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\python311\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.23.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in c:\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.5)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\python311\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\python311\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.3)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\python311\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python311\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccqFArDyKTDf"
   },
   "source": [
    "Applying the pipeline to every sentences creates a Document where every word is a Token object.\n",
    "\n",
    "Doc: https://spacy.io/api/doc\n",
    "\n",
    "Token: https://spacy.io/api/token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WcaeMUL2KTDg"
   },
   "outputs": [],
   "source": [
    "# Apply nlp pipeline to the column that has your sentences.\n",
    "data[\"tokenized\"] = data[\"customer_review\"].apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7i6ai1I8KTDg"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_review</th>\n",
       "      <th>NPS Score</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>London to Izmir via Istanbul. First time I'd ...</td>\n",
       "      <td>Passive</td>\n",
       "      <td>( , London, to, Izmir, via, Istanbul, ., First...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Istanbul to Bucharest. We make our check in i...</td>\n",
       "      <td>Detractor</td>\n",
       "      <td>( , Istanbul, to, Bucharest, ., We, make, our,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rome to Prishtina via Istanbul. I flew with t...</td>\n",
       "      <td>Detractor</td>\n",
       "      <td>( , Rome, to, Prishtina, via, Istanbul, ., I, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Flew on Turkish Airlines IAD-IST-KHI and retu...</td>\n",
       "      <td>Promoter</td>\n",
       "      <td>( , Flew, on, Turkish, Airlines, IAD, -, IST, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mumbai to Dublin via Istanbul. Never book Tur...</td>\n",
       "      <td>Detractor</td>\n",
       "      <td>( , Mumbai, to, Dublin, via, Istanbul, ., Neve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     customer_review  NPS Score  \\\n",
       "0   London to Izmir via Istanbul. First time I'd ...    Passive   \n",
       "1   Istanbul to Bucharest. We make our check in i...  Detractor   \n",
       "2   Rome to Prishtina via Istanbul. I flew with t...  Detractor   \n",
       "3   Flew on Turkish Airlines IAD-IST-KHI and retu...   Promoter   \n",
       "4   Mumbai to Dublin via Istanbul. Never book Tur...  Detractor   \n",
       "\n",
       "                                           tokenized  \n",
       "0  ( , London, to, Izmir, via, Istanbul, ., First...  \n",
       "1  ( , Istanbul, to, Bucharest, ., We, make, our,...  \n",
       "2  ( , Rome, to, Prishtina, via, Istanbul, ., I, ...  \n",
       "3  ( , Flew, on, Turkish, Airlines, IAD, -, IST, ...  \n",
       "4  ( , Mumbai, to, Dublin, via, Istanbul, ., Neve...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHRfZ2uEKTDh"
   },
   "source": [
    "A Token object has many attributes such as part-of-speech (pos_), lemma (lemma_), etc. Take a look at the documentation to see all attributes.\n",
    "\n",
    "The following function is an example on how you can fetch a specific pos tagging from a sentence. We return the lemmatization because we only want the infinitive word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qw0a_2ySyUo2"
   },
   "outputs": [],
   "source": [
    "# Create empty dataframes that will store your derived datasets\n",
    "derived_dataset1 = pd.DataFrame(columns=[\"Class\", \"pos\"])\n",
    "derived_dataset2 = pd.DataFrame(columns=[\"Class\", \"pos-np\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Yeak1tAOKTDi"
   },
   "outputs": [],
   "source": [
    "def get_pos(sentence, wanted_pos):  # wanted_pos refers to the desired pos tagging\n",
    "    words = []\n",
    "    for token in sentence:\n",
    "        if token.pos_ in wanted_pos:\n",
    "            words.append(token.lemma_)  # lemma returns a number. lemma_ return a string\n",
    "    return \" \".join(words)  # return value is as a string and not a list for countVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #f00; font-size: xx-large\">TODO: Explain the choice of wanted POS.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "147NRzwKKTDj"
   },
   "outputs": [],
   "source": [
    "derived_dataset1[\"Class\"] = data[\"NPS Score\"]\n",
    "derived_dataset1[\"pos\"] = data[\"tokenized\"].apply(lambda s: get_pos(s, [\"NOUN\", \"VERB\", \"ADJ\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "G_bUg_fVKTDk"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Passive</td>\n",
       "      <td>first time fly find good air cabin crew plane ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Detractor</td>\n",
       "      <td>make check airport take luggage go gate gate s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Detractor</td>\n",
       "      <td>fly company several time past year say get bad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Promoter</td>\n",
       "      <td>return maintain quality fly flight leave time ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Detractor</td>\n",
       "      <td>book turkish airline travel flight get delay h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Class                                                pos\n",
       "0    Passive  first time fly find good air cabin crew plane ...\n",
       "1  Detractor  make check airport take luggage go gate gate s...\n",
       "2  Detractor  fly company several time past year say get bad...\n",
       "3   Promoter  return maintain quality fly flight leave time ...\n",
       "4  Detractor  book turkish airline travel flight get delay h..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derived_dataset1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ne(sentence):\n",
    "    words = []\n",
    "    for entity in sentence.ents:\n",
    "        words.append(entity.text)\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "AuGv-NnfKTDj"
   },
   "outputs": [],
   "source": [
    "# Change this line to fetch your desired pos taggings for the second derived dataset\n",
    "derived_dataset2[\"Class\"] = data[\"NPS Score\"]\n",
    "derived_dataset2[\"pos-np\"] = data[\"tokenized\"].apply(lambda s: get_ne(s) + \" \" + get_pos(s, [\"VERB\", \"ADJ\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>pos-np</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Passive</td>\n",
       "      <td>London Izmir Istanbul First LHR Istanbul Engli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Detractor</td>\n",
       "      <td>Istanbul two two 5 the morning 2 hours first m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Detractor</td>\n",
       "      <td>Rome Prishtina Istanbul the past years Rome Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Promoter</td>\n",
       "      <td>Turkish Airlines Turkish Airlines first 2007 A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Detractor</td>\n",
       "      <td>Mumbai Dublin Istanbul Turkish Dublin Mumbai M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Class                                             pos-np\n",
       "0    Passive  London Izmir Istanbul First LHR Istanbul Engli...\n",
       "1  Detractor  Istanbul two two 5 the morning 2 hours first m...\n",
       "2  Detractor  Rome Prishtina Istanbul the past years Rome Pr...\n",
       "3   Promoter  Turkish Airlines Turkish Airlines first 2007 A...\n",
       "4  Detractor  Mumbai Dublin Istanbul Turkish Dublin Mumbai M..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derived_dataset2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "NR7AdW0MfXO6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "# For Derived Dataset 2, you also need to include Named Entities\n",
    "# Below is just an example of obtaining such entities on a specific sentence, but you would do NER\n",
    "#   on the dataset of your choice.\n",
    "# You can choose the types of entities (dates, organization, people) that you want,\n",
    "#   and then in your derived dataset, just make sure you include these entities separated by spaces (as shown for verbs)\n",
    "#   in a previous cell.\n",
    "\n",
    "sentence = \"apple is looking at buying U.K. startup for $1 billion\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pX4RgKhKTDk"
   },
   "source": [
    "Now that you have your derived datasets, you can move to perform your classificaton task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GhniwHtzfQt"
   },
   "source": [
    "## Perform Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the text as input features with associated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "data_vectorized = tfidf_vectorizer.fit_transform(data[\"tokenized\"].apply(str))\n",
    "derived_dataset1_vectorized = tfidf_vectorizer.fit_transform(derived_dataset1[\"pos\"])\n",
    "derived_dataset2_vectorized = tfidf_vectorizer.fit_transform(derived_dataset2[\"pos-np\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define 2 models using some default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "default_logistic_regression = LogisticRegression()\n",
    "default_multilayer_perceptron = MLPClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test/evaluate the 2 models with default parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Initialize KFold Cross Validator***\n",
    "\n",
    "Here we are initializing the `KFold` validator. Explanation of parameters:\n",
    "* `n_splits=4` means we are using 4-fold validation.\n",
    "* `shuffle=True` means data will be shuffled before spliting to batches.\n",
    "* `random_state` is the random seed used for shuffling. Here we are using a fixed number to keep reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "four_fold = KFold(n_splits=4, shuffle=True, random_state=4106)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Training and Evaluating the Model***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Utility Functions****\n",
    "\n",
    "Description for core function `train_and_evaluate`:\n",
    "\n",
    "Inputs:\n",
    "* `model`: The machine learning model to be trained and evaluated.\n",
    "* `validator`: A cross-validator object used for splitting the data into training and testing sets.\n",
    "* `csr_matrix_training`: A scipy `csr_matrix` containing input features for training the model.\n",
    "* `series_validating`: A pandas `Series` of target labels for validating the model's predictions.\n",
    "\n",
    "The function iterates through the training and testing sets created by the `validator`.\n",
    "1. For each iteration, it trains the model using the training sets.\n",
    "2. Then, it makes predictions on the test data.\n",
    "3. Precision and recall scores are calculated for both micro and macro averages using the `precision_score` and `recall_score` functions from scikit-learn.\n",
    "4. Zero division is handled by setting `zero_division=0`.\n",
    "\n",
    "The calculated precision and recall scores are appended to respective lists.\n",
    "\n",
    "After iterating through all folds, the function calculates the average precision and recall scores for both micro and macro averages using the helper function average.\n",
    "The function returns a tuple containing the average micro precision, average macro precision, average micro recall, and average macro recall scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def train_and_evaluate(model, validator, csr_matrix_training, series_validating):\n",
    "    micro_precision_scores = []\n",
    "    macro_precision_scores = []\n",
    "    micro_recall_scores = []\n",
    "    macro_recall_scores = []\n",
    "\n",
    "    X = csr_matrix_training\n",
    "    y = series_validating\n",
    "\n",
    "    # Split the dataset into training set and testing set with validator\n",
    "    for train_index, test_index in validator.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions ŷ\n",
    "        y_hat = model.predict(X_test)\n",
    "\n",
    "        # Calculate precision and recall for micro and macro averages\n",
    "        micro_precision = precision_score(y_test, y_hat, average=\"micro\", zero_division=0)\n",
    "        macro_precision = precision_score(y_test, y_hat, average=\"macro\", zero_division=0)\n",
    "        micro_recall = recall_score(y_test, y_hat, average=\"micro\", zero_division=0)\n",
    "        macro_recall = recall_score(y_test, y_hat, average=\"macro\", zero_division=0)\n",
    "\n",
    "        # Append scores to lists\n",
    "        if micro_precision != 0:\n",
    "            micro_precision_scores.append(micro_precision)\n",
    "        if macro_precision != 0:\n",
    "            macro_precision_scores.append(macro_precision)\n",
    "        if micro_recall != 0:\n",
    "            micro_recall_scores.append(micro_recall)\n",
    "        if macro_recall != 0:\n",
    "            macro_recall_scores.append(macro_recall)\n",
    "\n",
    "    # Calculate average precision and recall scores for micro and macro averages\n",
    "    return (\n",
    "        average(micro_precision_scores),\n",
    "        average(macro_precision_scores),\n",
    "        average(micro_recall_scores),\n",
    "        average(macro_recall_scores)\n",
    "    )\n",
    "\n",
    "\n",
    "def average(numeric_list):\n",
    "    return sum(numeric_list) / len(numeric_list)\n",
    "\n",
    "\n",
    "def print_result(\n",
    "    average_micro_precision,\n",
    "    average_macro_precision,\n",
    "    average_micro_recall,\n",
    "    average_macro_recall\n",
    "):\n",
    "    print(\"Average Micro Precision: {:.2f}\".format(average_micro_precision))\n",
    "    print(\"Average Macro Precision: {:.2f}\".format(average_macro_precision))\n",
    "    print(\"Average Micro Recall: {:.2f}\".format(average_micro_recall))\n",
    "    print(\"Average Macro Recall: {:.2f}\".format(average_macro_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Logistic Regression on Original Dataset****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'default_logistic_regression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\UO4_2\\CSI4106\\4106Assignments\\CSI4106-Fall-2023\\Assignment 4\\CSI4106_A4.ipynb Cell 40\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/UO4_2/CSI4106/4106Assignments/CSI4106-Fall-2023/Assignment%204/CSI4106_A4.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m default_lr_original_evaluation \u001b[39m=\u001b[39m train_and_evaluate(\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/UO4_2/CSI4106/4106Assignments/CSI4106-Fall-2023/Assignment%204/CSI4106_A4.ipynb#X54sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     default_logistic_regression,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/UO4_2/CSI4106/4106Assignments/CSI4106-Fall-2023/Assignment%204/CSI4106_A4.ipynb#X54sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     four_fold, data_vectorized,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/UO4_2/CSI4106/4106Assignments/CSI4106-Fall-2023/Assignment%204/CSI4106_A4.ipynb#X54sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     data[\u001b[39m\"\u001b[39m\u001b[39mNPS Score\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/UO4_2/CSI4106/4106Assignments/CSI4106-Fall-2023/Assignment%204/CSI4106_A4.ipynb#X54sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/UO4_2/CSI4106/4106Assignments/CSI4106-Fall-2023/Assignment%204/CSI4106_A4.ipynb#X54sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m print_result(\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/UO4_2/CSI4106/4106Assignments/CSI4106-Fall-2023/Assignment%204/CSI4106_A4.ipynb#X54sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m*\u001b[39mdefault_lr_original_evaluation\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/UO4_2/CSI4106/4106Assignments/CSI4106-Fall-2023/Assignment%204/CSI4106_A4.ipynb#X54sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'default_logistic_regression' is not defined"
     ]
    }
   ],
   "source": [
    "default_lr_original_evaluation = train_and_evaluate(\n",
    "    default_logistic_regression,\n",
    "    four_fold, data_vectorized,\n",
    "    data[\"NPS Score\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *default_lr_original_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Multilayer Perceptron on Original Dataset****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Micro Precision: 0.77\n",
      "Average Macro Precision: 0.68\n",
      "Average Micro Recall: 0.77\n",
      "Average Macro Recall: 0.68\n"
     ]
    }
   ],
   "source": [
    "default_mlp_original_evaluation = train_and_evaluate(\n",
    "    default_multilayer_perceptron,\n",
    "    four_fold, data_vectorized,\n",
    "    data[\"NPS Score\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *default_mlp_original_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Logistic Regression on Derived Dataset #1****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Micro Precision: 0.79\n",
      "Average Macro Precision: 0.70\n",
      "Average Micro Recall: 0.79\n",
      "Average Macro Recall: 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "default_lr_derived_1_evaluation = train_and_evaluate(\n",
    "    default_logistic_regression,\n",
    "    four_fold, derived_dataset1_vectorized,\n",
    "    derived_dataset1[\"Class\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *default_lr_derived_1_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Multilayer Perceptron on Derived Dataset #1****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Micro Precision: 0.74\n",
      "Average Macro Precision: 0.66\n",
      "Average Micro Recall: 0.74\n",
      "Average Macro Recall: 0.66\n"
     ]
    }
   ],
   "source": [
    "default_mlp_derived_1_evaluation = train_and_evaluate(\n",
    "    default_multilayer_perceptron,\n",
    "    four_fold, derived_dataset1_vectorized,\n",
    "    derived_dataset1[\"Class\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *default_mlp_derived_1_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Logistic Regression on Derived Dataset #2****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Micro Precision: 0.78\n",
      "Average Macro Precision: 0.71\n",
      "Average Micro Recall: 0.78\n",
      "Average Macro Recall: 0.67\n"
     ]
    }
   ],
   "source": [
    "default_lr_derived_2_evaluation = train_and_evaluate(\n",
    "    default_logistic_regression,\n",
    "    four_fold, derived_dataset2_vectorized,\n",
    "    derived_dataset2[\"Class\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *default_lr_derived_2_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Multilayer Perceptron on Derived Dataset #2****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Micro Precision: 0.73\n",
      "Average Macro Precision: 0.65\n",
      "Average Micro Recall: 0.73\n",
      "Average Macro Recall: 0.65\n"
     ]
    }
   ],
   "source": [
    "default_mlp_derived_2_evaluation = train_and_evaluate(\n",
    "    default_multilayer_perceptron,\n",
    "    four_fold, derived_dataset2_vectorized,\n",
    "    derived_dataset2[\"Class\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *default_mlp_derived_2_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test/evaluate the 2 models with modified parameters (#1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_solver_multilayer_perceptron = MLPClassifier(solver='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Micro Precision: 0.75\n",
      "Average Macro Precision: 0.50\n",
      "Average Micro Recall: 0.75\n",
      "Average Macro Recall: 0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sgd_solver_mlp_original_evaluation = train_and_evaluate(\n",
    "    sgd_solver_multilayer_perceptron,\n",
    "    four_fold, data_vectorized,\n",
    "    data[\"NPS Score\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *sgd_solver_mlp_original_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Micro Precision: 0.76\n",
      "Average Macro Precision: 0.50\n",
      "Average Micro Recall: 0.76\n",
      "Average Macro Recall: 0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sgd_solver_mlp_derived_1_evaluation = train_and_evaluate(\n",
    "    sgd_solver_multilayer_perceptron,\n",
    "    four_fold, derived_dataset1_vectorized,\n",
    "    derived_dataset1[\"Class\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *sgd_solver_mlp_derived_1_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Micro Precision: 0.76\n",
      "Average Macro Precision: 0.50\n",
      "Average Micro Recall: 0.76\n",
      "Average Macro Recall: 0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sgd_solver_mlp_derived_2_evaluation = train_and_evaluate(\n",
    "    sgd_solver_multilayer_perceptron,\n",
    "    four_fold, derived_dataset2_vectorized,\n",
    "    derived_dataset2[\"Class\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *sgd_solver_mlp_derived_2_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test/evaluate the 2 models with modified parameters (#2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_init_learning_rate_multilayer_perceptron = MLPClassifier(learning_rate_init=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Micro Precision: 0.76\n",
      "Average Macro Precision: 0.68\n",
      "Average Micro Recall: 0.76\n",
      "Average Macro Recall: 0.68\n"
     ]
    }
   ],
   "source": [
    "new_mlp_original_evaluation = train_and_evaluate(\n",
    "    larger_init_learning_rate_multilayer_perceptron,\n",
    "    four_fold, data_vectorized,\n",
    "    data[\"NPS Score\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *new_mlp_original_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Micro Precision: 0.76\n",
      "Average Macro Precision: 0.68\n",
      "Average Micro Recall: 0.76\n",
      "Average Macro Recall: 0.68\n"
     ]
    }
   ],
   "source": [
    "new_mlp_derived_1_evaluation = train_and_evaluate(\n",
    "    larger_init_learning_rate_multilayer_perceptron,\n",
    "    four_fold, derived_dataset1_vectorized,\n",
    "    derived_dataset1[\"Class\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *new_mlp_derived_1_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Micro Precision: 0.75\n",
      "Average Macro Precision: 0.68\n",
      "Average Micro Recall: 0.75\n",
      "Average Macro Recall: 0.67\n"
     ]
    }
   ],
   "source": [
    "new_mlp_derived_2_evaluation = train_and_evaluate(\n",
    "    larger_init_learning_rate_multilayer_perceptron,\n",
    "    four_fold, derived_dataset2_vectorized,\n",
    "    derived_dataset2[\"Class\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *new_mlp_derived_2_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the obtained results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
