{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "Pbcc9QS1tnBN",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# ASSIGNMENT 4 - Classification Empirical Study: Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2Eeke4Z_EkW"
   },
   "source": [
    "## Group Description\n",
    "\n",
    "Group Number: 25\n",
    "\n",
    "Member Name 1: Jake Wang\n",
    "\n",
    "Member Student Number 1: ***REMOVED***\n",
    "\n",
    "Member Name 2: Victor Li\n",
    "\n",
    "Member Student Number 2: ***REMOVED***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-so3pwbPKTDX"
   },
   "source": [
    "This notebook is a starting point for Assignment 4. In this assignment, you will perform a classification empirical study. This notebook will help you to create derived datasets in Section 2 of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhFvS3q7Lu0R"
   },
   "outputs": [],
   "source": [
    "# Let's start by installing spaCy\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCWgl6PLKTDY"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EX9WQWGSwU2D"
   },
   "source": [
    "You have been given a list of datasets in the assignment description. Choose one of the datasets and provide the link below and read the dataset using pandas. You should provide a link to your own Github repository even if you are using a reduced version of a dataset from your TA's repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSyg0jjC1jJa"
   },
   "source": [
    "## Description of Chosen Dataset and Classification Task\n",
    "We decided to use the \"Airliner Passenger Reviews\" dataset. The dataset is coming from [Kaggle](https://www.kaggle.com/). It can be retrieved by following [this link](https://www.kaggle.com/datasets/malharkhatu/airline-passenger-reviews).\n",
    "\n",
    "### Objective\n",
    "The objective of this study is to leverage NLP techniques to analyze and classify Airline Passenger Reviews, focusing on sentiment analysis. The primary goal is to understand the underlying sentiments expressed in customer reviews and predict whether the sentiment falls into categories of \"Promoter,\" \"Detractor,\" or \"Passive\" based on the Net Promoter Score (NPS).\n",
    "\n",
    "### Dataset Description\n",
    "The dataset includes an extensive samples of airline passenger reviews, each of which is classified into three distinct categories: Promoters, Detractors, and Passives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Xx4qMCLKTDb"
   },
   "outputs": [],
   "source": [
    "# url = \"https://raw.githubusercontent.com/uOttawa-Collabs/CSI4106-Fall-2023/master/Assignment%204/reduced_drugsComTest_raw_fiveclasses.csv\"\n",
    "url = \"https://raw.githubusercontent.com/uOttawa-Collabs/CSI4106-Fall-2023/master/Assignment%204/reduced_file_AirPassengerReviews.csv\"\n",
    "# url = \"https://raw.githubusercontent.com/uOttawa-Collabs/CSI4106-Fall-2023/master/Assignment%204/reduced_file_cnnnews.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wg24OUV81Xgm"
   },
   "outputs": [],
   "source": [
    "print(url)\n",
    "data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQ5nSY1HKTDd"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3pycllPKTDd"
   },
   "source": [
    "This is where you create the NLP pipeline. load() will download the correct model (English)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQtSi8XuKTDe"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccqFArDyKTDf"
   },
   "source": [
    "Applying the pipeline to every sentences creates a Document where every word is a Token object.\n",
    "\n",
    "Doc: https://spacy.io/api/doc\n",
    "\n",
    "Token: https://spacy.io/api/token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WcaeMUL2KTDg"
   },
   "outputs": [],
   "source": [
    "# Apply nlp pipeline to the column that has your sentences.\n",
    "data[\"tokenized\"] = data[\"customer_review\"].apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7i6ai1I8KTDg"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHRfZ2uEKTDh"
   },
   "source": [
    "A Token object has many attributes such as part-of-speech (pos_), lemma (lemma_), etc. Take a look at the documentation to see all attributes.\n",
    "\n",
    "The following function is an example on how you can fetch a specific pos tagging from a sentence. We return the lemmatization because we only want the infinitive word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qw0a_2ySyUo2"
   },
   "outputs": [],
   "source": [
    "# Create empty dataframes that will store your derived datasets\n",
    "derived_dataset1 = pd.DataFrame(columns=[\"Class\", \"pos\"])\n",
    "derived_dataset2 = pd.DataFrame(columns=[\"Class\", \"pos-np\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yeak1tAOKTDi"
   },
   "outputs": [],
   "source": [
    "def get_pos(sentence, wanted_pos):  # wanted_pos refers to the desired pos tagging\n",
    "    words = []\n",
    "    for token in sentence:\n",
    "        if token.pos_ in wanted_pos:\n",
    "            words.append(token.lemma_)  # lemma returns a number. lemma_ return a string\n",
    "    return \" \".join(words)  # return value is as a string and not a list for countVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `derived_dataset1` contains lemmatized nouns, verbs and adjectives. We chose tokens that bear these POS for the following reason:\n",
    "    * Nouns are crucial for understanding the main subjects or objects in the review, providing insights into the aspects of the airline service that passengers are commenting on.\n",
    "    * Verbs can reveal the actions or events described by passengers, helping to discern the nature of their experiences and sentiments.\n",
    "    * Adjectives contribute to understanding the sentiment and subjective qualities of the review, offering insights into the passengers' feelings and evaluations, which is, actually, the most important POS in terms of the classification task.\n",
    "* `derived_dataset2` is almost the same as `derived_dataset1`, but with nouns removed, as `derived_dataset2` already contains recognized named entities, which are all nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "147NRzwKKTDj"
   },
   "outputs": [],
   "source": [
    "derived_dataset1[\"Class\"] = data[\"NPS Score\"]\n",
    "derived_dataset1[\"pos\"] = data[\"tokenized\"].apply(lambda s: get_pos(s, [\"NOUN\", \"VERB\", \"ADJ\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_bUg_fVKTDk"
   },
   "outputs": [],
   "source": [
    "derived_dataset1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ne(sentence):\n",
    "    words = []\n",
    "    for entity in sentence.ents:\n",
    "        words.append(entity.text)\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuGv-NnfKTDj"
   },
   "outputs": [],
   "source": [
    "# Change this line to fetch your desired pos taggings for the second derived dataset\n",
    "derived_dataset2[\"Class\"] = data[\"NPS Score\"]\n",
    "derived_dataset2[\"pos-np\"] = data[\"tokenized\"].apply(lambda s: get_ne(s) + \" \" + get_pos(s, [\"VERB\", \"ADJ\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derived_dataset2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NR7AdW0MfXO6"
   },
   "outputs": [],
   "source": [
    "# For Derived Dataset 2, you also need to include Named Entities\n",
    "# Below is just an example of obtaining such entities on a specific sentence, but you would do NER\n",
    "#   on the dataset of your choice.\n",
    "# You can choose the types of entities (dates, organization, people) that you want,\n",
    "#   and then in your derived dataset, just make sure you include these entities separated by spaces (as shown for verbs)\n",
    "#   in a previous cell.\n",
    "\n",
    "sentence = \"apple is looking at buying U.K. startup for $1 billion\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pX4RgKhKTDk"
   },
   "source": [
    "Now that you have your derived datasets, you can move to perform your classificaton task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GhniwHtzfQt"
   },
   "source": [
    "## Perform Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the text as input features with associated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "data_vectorized = tfidf_vectorizer.fit_transform(data[\"tokenized\"].apply(str))\n",
    "derived_dataset1_vectorized = tfidf_vectorizer.fit_transform(derived_dataset1[\"pos\"])\n",
    "derived_dataset2_vectorized = tfidf_vectorizer.fit_transform(derived_dataset2[\"pos-np\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define 2 models using some default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "default_logistic_regression = LogisticRegression()\n",
    "default_multilayer_perceptron = MLPClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test/evaluate the 2 models with default parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Initialize KFold Cross Validator***\n",
    "\n",
    "Here we are initializing the `KFold` validator. Explanation of parameters:\n",
    "* `n_splits=4` means we are using 4-fold validation.\n",
    "* `shuffle=True` means data will be shuffled before spliting to batches.\n",
    "* `random_state` is the random seed used for shuffling. Here we are using a fixed number to keep reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "four_fold = KFold(n_splits=4, shuffle=True, random_state=4106)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Training and Evaluating the Model***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Utility Functions****\n",
    "\n",
    "Description for core function `train_and_evaluate`:\n",
    "\n",
    "Inputs:\n",
    "* `model`: The machine learning model to be trained and evaluated.\n",
    "* `validator`: A cross-validator object used for splitting the data into training and testing sets.\n",
    "* `csr_matrix_training`: A scipy `csr_matrix` containing input features for training the model.\n",
    "* `series_validating`: A pandas `Series` of target labels for validating the model's predictions.\n",
    "\n",
    "The function iterates through the training and testing sets created by the `validator`.\n",
    "1. For each iteration, it trains the model using the training sets.\n",
    "2. Then, it makes predictions on the test data.\n",
    "3. Precision and recall scores are calculated for both micro and macro averages using the `precision_score` and `recall_score` functions from scikit-learn.\n",
    "4. Zero division is handled by setting `zero_division=0`.\n",
    "\n",
    "The calculated precision and recall scores are appended to respective lists.\n",
    "\n",
    "After iterating through all folds, the function calculates the average precision and recall scores for both micro and macro averages using the helper function average.\n",
    "The function returns a tuple containing the average micro precision, average macro precision, average micro recall, and average macro recall scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def train_and_evaluate(model, validator, csr_matrix_training, series_validating):\n",
    "    micro_precision_scores = []\n",
    "    macro_precision_scores = []\n",
    "    micro_recall_scores = []\n",
    "    macro_recall_scores = []\n",
    "\n",
    "    X = csr_matrix_training\n",
    "    y = series_validating\n",
    "\n",
    "    # Split the dataset into training set and testing set with validator\n",
    "    for train_index, test_index in validator.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions ŷ\n",
    "        y_hat = model.predict(X_test)\n",
    "\n",
    "        # Calculate precision and recall for micro and macro averages\n",
    "        micro_precision = precision_score(y_test, y_hat, average=\"micro\", zero_division=0)\n",
    "        macro_precision = precision_score(y_test, y_hat, average=\"macro\", zero_division=0)\n",
    "        micro_recall = recall_score(y_test, y_hat, average=\"micro\", zero_division=0)\n",
    "        macro_recall = recall_score(y_test, y_hat, average=\"macro\", zero_division=0)\n",
    "\n",
    "        # Append scores to lists\n",
    "        if micro_precision != 0:\n",
    "            micro_precision_scores.append(micro_precision)\n",
    "        if macro_precision != 0:\n",
    "            macro_precision_scores.append(macro_precision)\n",
    "        if micro_recall != 0:\n",
    "            micro_recall_scores.append(micro_recall)\n",
    "        if macro_recall != 0:\n",
    "            macro_recall_scores.append(macro_recall)\n",
    "\n",
    "    # Calculate average precision and recall scores for micro and macro averages\n",
    "    return (\n",
    "        average(micro_precision_scores),\n",
    "        average(macro_precision_scores),\n",
    "        average(micro_recall_scores),\n",
    "        average(macro_recall_scores)\n",
    "    )\n",
    "\n",
    "\n",
    "def average(numeric_list):\n",
    "    return sum(numeric_list) / len(numeric_list)\n",
    "\n",
    "\n",
    "def print_result(\n",
    "    average_micro_precision,\n",
    "    average_macro_precision,\n",
    "    average_micro_recall,\n",
    "    average_macro_recall\n",
    "):\n",
    "    print(\"Average Micro Precision: {:.2f}\".format(average_micro_precision))\n",
    "    print(\"Average Macro Precision: {:.2f}\".format(average_macro_precision))\n",
    "    print(\"Average Micro Recall: {:.2f}\".format(average_micro_recall))\n",
    "    print(\"Average Macro Recall: {:.2f}\".format(average_macro_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Logistic Regression on Original Dataset****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_lr_original_evaluation = train_and_evaluate(\n",
    "    default_logistic_regression,\n",
    "    four_fold, data_vectorized,\n",
    "    data[\"NPS Score\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *default_lr_original_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Multilayer Perceptron on Original Dataset****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_mlp_original_evaluation = train_and_evaluate(\n",
    "    default_multilayer_perceptron,\n",
    "    four_fold, data_vectorized,\n",
    "    data[\"NPS Score\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *default_mlp_original_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Logistic Regression on Derived Dataset #1****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_lr_derived_1_evaluation = train_and_evaluate(\n",
    "    default_logistic_regression,\n",
    "    four_fold, derived_dataset1_vectorized,\n",
    "    derived_dataset1[\"Class\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *default_lr_derived_1_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Multilayer Perceptron on Derived Dataset #1****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_mlp_derived_1_evaluation = train_and_evaluate(\n",
    "    default_multilayer_perceptron,\n",
    "    four_fold, derived_dataset1_vectorized,\n",
    "    derived_dataset1[\"Class\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *default_mlp_derived_1_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Logistic Regression on Derived Dataset #2****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_lr_derived_2_evaluation = train_and_evaluate(\n",
    "    default_logistic_regression,\n",
    "    four_fold, derived_dataset2_vectorized,\n",
    "    derived_dataset2[\"Class\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *default_lr_derived_2_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Multilayer Perceptron on Derived Dataset #2****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_mlp_derived_2_evaluation = train_and_evaluate(\n",
    "    default_multilayer_perceptron,\n",
    "    four_fold, derived_dataset2_vectorized,\n",
    "    derived_dataset2[\"Class\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *default_mlp_derived_2_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test/evaluate the 2 models with modified parameters (#1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first modified parameter was the type of the solver.\n",
    "We changed the solver to 'sgd' to test the performances of the model in three datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new MLP model with sgd solver\n",
    "sgd_solver_multilayer_perceptron = MLPClassifier(solver='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****sgd solver Multilayer Perceptron on Original Dataset****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_solver_mlp_original_evaluation = train_and_evaluate(\n",
    "    sgd_solver_multilayer_perceptron,\n",
    "    four_fold, data_vectorized,\n",
    "    data[\"NPS Score\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *sgd_solver_mlp_original_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****sgd solver Multilayer Perceptron on Derived Dataset #1****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_solver_mlp_derived_1_evaluation = train_and_evaluate(\n",
    "    sgd_solver_multilayer_perceptron,\n",
    "    four_fold, derived_dataset1_vectorized,\n",
    "    derived_dataset1[\"Class\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *sgd_solver_mlp_derived_1_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****sgd solver Multilayer Perceptron on Derived Dataset #2****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_solver_mlp_derived_2_evaluation = train_and_evaluate(\n",
    "    sgd_solver_multilayer_perceptron,\n",
    "    four_fold, derived_dataset2_vectorized,\n",
    "    derived_dataset2[\"Class\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *sgd_solver_mlp_derived_2_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test/evaluate the 2 models with modified parameters (#2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second modified parameter was the initial learning rate.\n",
    "We increased value of the initial learning rate from 0.001 to 0.05 to observe the impact the results on different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new MLP model which the initial learning rate is 0.05\n",
    "larger_init_learning_rate_multilayer_perceptron = MLPClassifier(learning_rate_init=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****High init learning rate Multilayer Perceptron on Original Dataset****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mlp_original_evaluation = train_and_evaluate(\n",
    "    larger_init_learning_rate_multilayer_perceptron,\n",
    "    four_fold, data_vectorized,\n",
    "    data[\"NPS Score\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *new_mlp_original_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****High init learning rate Multilayer Perceptron on Derived Dataset #1****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mlp_derived_1_evaluation = train_and_evaluate(\n",
    "    larger_init_learning_rate_multilayer_perceptron,\n",
    "    four_fold, derived_dataset1_vectorized,\n",
    "    derived_dataset1[\"Class\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *new_mlp_derived_1_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****High init learning rate Multilayer Perceptron on Derived Dataset #2****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mlp_derived_2_evaluation = train_and_evaluate(\n",
    "    larger_init_learning_rate_multilayer_perceptron,\n",
    "    four_fold, derived_dataset2_vectorized,\n",
    "    derived_dataset2[\"Class\"]\n",
    ")\n",
    "\n",
    "print_result(\n",
    "    *new_mlp_derived_2_evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the obtained results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made a comparison table of all models for each dataset. The table has been shown below and the code must be executed after running all above code.\n",
    "\n",
    "When we modified parameters of the MLP model, we have tried all options of many parameters including hidden_layer_sizes, activation, solver, learning rate, learning rate init, and tol... However, most of them did not cause significant impact on the performance of the model. The differences of precisions and recalls were only 0.01 or 0.02.\n",
    "For example, when the hidden layer sizes are 50, 100, 150, and 200, there is no difference.\n",
    "\n",
    "In the explored parameters, we found 2 parameters which have significant impacts: the solver, and the initial learning rate. When the solver type is sgd, the macro precision and macro recall is lower than other models, where the macro precision is much lower. We consider that the results with SGD may be attributed by adjusting other parameters to improve its performance, such as power_t, early_stopping, and n_iter_no_change.Moreover, it's worth mentioning that when the solver type is 'lbfgs' instead of the default 'adam,' there is no significant difference in the performance of the two models. Although the note in the official documentation mentions that the default solver 'adam' works well on relatively large datasets, while 'lbfgs' can converge faster and perform better on small datasets, we did not observe this difference.\n",
    "\n",
    "In addition, when we increased the initial learning rate from the default 0.001 to 0.05, there was some improvement in the four evaluations across the three datasets, although these improvements were not particularly obvious. It means that 0.001 initial learning rate seems a bit low for our 3 datasets. This not only results in the model over-training but also requires more training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plot\n",
    "\n",
    "name_list = [\n",
    "    \"Default Multilayer Perceptron\", \"Logistic Regression Model\",\n",
    "    \"SGD Multilayer Perceptron\", \"High Rate Multilayer Perceptron\"\n",
    "]\n",
    "x_label = [\"Micro Precision\", \"Macro Precision\", \"Micro Recall\", \"Macro Recall\"]\n",
    "\n",
    "# the results of original dataset\n",
    "original_result_list = []\n",
    "original_result_list.append(default_mlp_original_evaluation)\n",
    "original_result_list.append(default_lr_original_evaluation)\n",
    "original_result_list.append(sgd_solver_mlp_original_evaluation)\n",
    "original_result_list.append(new_mlp_original_evaluation)\n",
    "\n",
    "# the results of derived dataset 1\n",
    "derived_1_result_list = []\n",
    "derived_1_result_list.append(default_mlp_derived_1_evaluation)\n",
    "derived_1_result_list.append(default_lr_derived_1_evaluation)\n",
    "derived_1_result_list.append(sgd_solver_mlp_derived_1_evaluation)\n",
    "derived_1_result_list.append(new_mlp_derived_1_evaluation)\n",
    "\n",
    "# the results of derived dataset 1\n",
    "derived_2_result_list = []\n",
    "derived_2_result_list.append(default_mlp_derived_2_evaluation)\n",
    "derived_2_result_list.append(default_lr_derived_2_evaluation)\n",
    "derived_2_result_list.append(sgd_solver_mlp_derived_2_evaluation)\n",
    "derived_2_result_list.append(new_mlp_derived_2_evaluation)\n",
    "\n",
    "\n",
    "width = 0.3\n",
    "x_list = [0.2, 2.8, 5.4, 8]\n",
    "plot.figure(figsize=(12, 4))\n",
    "for i in range(len(original_result_list)):\n",
    "    plot.bar(x_list, original_result_list[i], width=0.3, tick_label=x_label, label=name_list[i]) if i==2 else plot.bar(x_list, original_result_list[i], width=0.3, label=name_list[i])\n",
    "\n",
    "    for k in range(len(x_list)):\n",
    "        x_list[k] = x_list[k] + width\n",
    "plot.subplots_adjust(right=2, top=2)\n",
    "plot.xlabel(\"The results of different models on the original dataset\")\n",
    "plot.ylabel(\"The percentages of values\")\n",
    "plot.legend()\n",
    "plot.show()\n",
    "\n",
    "for i in range(len(derived_1_result_list)):\n",
    "    plot.bar(x_list, derived_1_result_list[i], width=0.3, tick_label=x_label, label=name_list[i]) if i==2 else plot.bar(x_list, derived_1_result_list[i], width=0.3, label=name_list[i])\n",
    "\n",
    "    for k in range(len(x_list)):\n",
    "        x_list[k] = x_list[k] + width\n",
    "plot.subplots_adjust(right=2, top=2)\n",
    "plot.xlabel(\"The results of different models on the derived dataset 1\")\n",
    "plot.ylabel(\"The percentages of values\")\n",
    "plot.legend()\n",
    "plot.show()\n",
    "\n",
    "for i in range(len(derived_2_result_list)):\n",
    "    plot.bar(x_list, derived_2_result_list[i], width=0.3, tick_label=x_label, label=name_list[i]) if i==2 else plot.bar(x_list, derived_2_result_list[i], width=0.3, label=name_list[i])\n",
    "\n",
    "    for k in range(len(x_list)):\n",
    "        x_list[k] = x_list[k] + width\n",
    "plot.subplots_adjust(right=2, top=2)\n",
    "plot.xlabel(\"The results of different models on the derived dataset 2\")\n",
    "plot.ylabel(\"The percentages of values\")\n",
    "plot.legend()\n",
    "plot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]\tF. Pedregosa et al., ‘Scikit-learn: Machine Learning in Python’, Journal of Machine Learning Research, vol. 12, pp. 2825–2830, 2011.\n",
    "\n",
    "[2]\tL. Buitinck et al., ‘API design for machine learning software: experiences from the scikit-learn project’, in ECML PKDD Workshop: Languages for Data Mining and Machine Learning, 2013, pp. 108–122.\n",
    "References\n",
    "\n",
    "[3] M. Khatu, Airline Passenger Reviews, https://www.kaggle.com/datasets/malharkhatu/airline-passenger-reviews (accessed Dec. 3, 2023).\n",
    "\n",
    "[4] spaCy, “Library architecture · spacy API documentation,” Library Architecture, https://spacy.io/api (accessed Dec. 3, 2023)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
